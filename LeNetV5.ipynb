{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet for Traffic Sign V5\n",
    "![LeNet Architecture](lenet.png)\n",
    "Modified from source: Yan LeCun\n",
    "\n",
    "Author: Peng \"Patrick\" Su\n",
    "Augmenting the under-sampled data, to make it better-balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the traffic sign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = '../traffic-signs-data/train.p'\n",
    "validation_file = '../traffic-signs-data/valid.p'\n",
    "testing_file = '../traffic-signs-data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traffic sign data comes as 32x32x3 images, and LeNet accepts 32x32xC images. No need to pad anything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Shuffle the training data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 2 has the max amount of data, 2010\n",
      "Label 0 has the min amount of data, 180\n",
      "Next, Let's augment the undersampled classes to the max count\n"
     ]
    }
   ],
   "source": [
    "y_train_DF = pd.DataFrame(y_train, columns=['label'])\n",
    "group_count = y_train_DF.groupby(['label']).size()\n",
    "print('Label {} has the max amount of data, {}'.format(np.argmax(group_count), np.max(group_count)))\n",
    "print('Label {} has the min amount of data, {}'.format(np.argmin(group_count), np.min(group_count)))\n",
    "print(\"Next, Let's augment the undersampled classes to the max count\")\n",
    "max_count = np.max(group_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage import transform, filters, exposure\n",
    "import cv2\n",
    "def augment_image(img,angle_range=(-5, 5),shear_range=10,trans_range=5,brightness=0):\n",
    "    # rotation\n",
    "    angle = np.random.uniform(*angle_range)\n",
    "    rows, cols, _ = img.shape\n",
    "    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2), angle, 1)\n",
    "    # Translation\n",
    "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "    tr_y = trans_range*np.random.uniform()-trans_range/2\n",
    "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "    # Shear\n",
    "    pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "    pt1 = 5+shear_range*np.random.uniform()-shear_range/2\n",
    "    pt2 = 20+shear_range*np.random.uniform()-shear_range/2\n",
    "    # Brightness\n",
    "    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
    "    shear_M = cv2.getAffineTransform(pts1,pts2)\n",
    "\n",
    "    img = cv2.warpAffine(img,Rot_M,(cols,rows))\n",
    "    img = cv2.warpAffine(img,Trans_M,(cols,rows))\n",
    "    img = cv2.warpAffine(img,shear_M,(cols,rows))\n",
    "\n",
    "    if brightness == 1:\n",
    "        img = augment_brightness_camera_images(img)\n",
    "    \n",
    "    return img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X, y, group_count):\n",
    "    augmented_X = np.zeros([max_count*43, *X.shape[1:]], dtype = np.uint8)\n",
    "    augmented_y = np.zeros([max_count*43, *y.shape[1:]], dtype = np.uint8)\n",
    "    for label,count in enumerate(group_count):\n",
    "        perLabelData = X[y==label]\n",
    "        augmented_X[label*max_count:label*max_count+count] = perLabelData\n",
    "        for i in range(max_count - count):\n",
    "            ind = np.random.randint(0, count)\n",
    "#             new_img = augment_image(perLabelData[ind])\n",
    "#             augmented_X[label*max_count+count+i] = new_img\n",
    "            augmented_X[label*max_count+count+i] = perLabelData[ind]\n",
    "        augmented_y[label*max_count:label*max_count+max_count] = label  \n",
    "    return augmented_X, augmented_y\n",
    "X_train, y_train = augment_data(X_train, y_train, group_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86430, 32, 32, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    return (img - 128.)/128.\n",
    "def hist_eq(img):\n",
    "    if (len(img.shape)>3): # if you're passing in a collection of images\n",
    "        num_images = img.shape[0]\n",
    "        image_shape = img.shape[1:]\n",
    "        locEqImg = np.zeros([num_images,image_shape[0],image_shape[1],image_shape[2]])\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(6,6))\n",
    "        for i in range(num_images):\n",
    "            currImg = img[i].squeeze()\n",
    "            img_lab = cv2.cvtColor(currImg, cv2.COLOR_RGB2LAB)\n",
    "            img_lab[:,:,0]=clahe.apply(img_lab[:,:,0])\n",
    "            tmp = cv2.cvtColor(img_lab, cv2.COLOR_LAB2RGB)\n",
    "            locEqImg[i] = tmp\n",
    "    else:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(6,6))\n",
    "        img_lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        img_lab[:,:,0]=clahe.apply(img_lab[:,:,0])\n",
    "        tmp = cv2.cvtColor(img_lab, cv2.COLOR_LAB2RGB)\n",
    "        locEqImg = tmp\n",
    "    return locEqImg\n",
    "\n",
    "X_train = hist_eq(X_train)\n",
    "X_train = normalize(X_train)\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "X_valid = hist_eq(X_valid)\n",
    "X_valid = normalize(X_valid)\n",
    "X_valid = X_valid.astype('float32')\n",
    "\n",
    "X_test = hist_eq(X_test)\n",
    "X_test = normalize(X_test)\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86430, 32, 32, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TensorFlow\n",
    "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implement LeNet-5\n",
    "Implement the [LeNet-5](http://yann.lecun.com/exdb/lenet/) neural network architecture.\n",
    "\n",
    "This is the only cell you need to edit.\n",
    "### Input\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "### Architecture\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using `tf.contrib.layers.flatten`, which is already imported for you.\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 5: Fully Connected (Logits).** This should have 10 outputs.\n",
    "\n",
    "### Output\n",
    "Return the result of the 2nd fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    weights = {\n",
    "        'wc1': tf.Variable(tf.truncated_normal([5,5,3,6], mu, sigma)),\n",
    "        'wc2': tf.Variable(tf.truncated_normal([5,5,6,16], mu, sigma)),\n",
    "        'wf3': tf.Variable(tf.truncated_normal([5*5*16, 120], mu, sigma)),\n",
    "        'wf4': tf.Variable(tf.truncated_normal([120, 84], mu, sigma)),\n",
    "        'wf5': tf.Variable(tf.truncated_normal([84, 43], mu, sigma))}\n",
    "    biases = {\n",
    "        'bc1': tf.zeros([6]),\n",
    "        'bc2': tf.zeros([16]),\n",
    "        'bf3': tf.zeros([120]),\n",
    "        'bf4': tf.zeros([84]),\n",
    "        'bf5': tf.zeros([43]),\n",
    "    }\n",
    "    # TODO: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1 = tf.nn.conv2d(x, weights['wc1'], strides = [1, 1, 1, 1], padding = 'VALID')\n",
    "    conv1 = tf.nn.bias_add(conv1, biases['bc1'])\n",
    "    # TODO: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)    \n",
    "    # TODO: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'VALID')\n",
    "    conv1 = tf.nn.dropout(conv1, keep_prob)\n",
    "#     flat1 = flatten(conv1, [-1, 14*14*6])\n",
    "    \n",
    "    # TODO: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2 = tf.nn.conv2d(conv1, weights['wc2'], strides = [1, 1, 1, 1], padding = 'VALID')\n",
    "    conv2 = tf.nn.bias_add(conv2, biases['bc2'])    \n",
    "    # TODO: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    # TODO: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'VALID')\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "    # TODO: Flatten. Input = 5x5x8. Output = 200.\n",
    "\n",
    "#     flat2 = flatten(conv2, [-1, 5*5*16])\n",
    "#     flat = tf.concat(1, [flat1, flat2])\n",
    "    \n",
    "    flat = flatten(conv2, [-1, 5*5*16])\n",
    "    \n",
    "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc3 = tf.add(tf.matmul(flat, weights['wf3']), biases['bf3'])\n",
    "    # TODO: Activation.\n",
    "    fc3 = tf.nn.relu(fc3)\n",
    "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc4 = tf.add(tf.matmul(fc3, weights['wf4']), biases['bf4'])\n",
    "    # TODO: Activation.\n",
    "    fc4 = tf.nn.relu(fc4)\n",
    "    # TODO: Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    logits = tf.add(tf.matmul(fc4, weights['wf5']), biases['bf5'])\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "Train LeNet to classify traffic sign data.\n",
    "\n",
    "`x` is a placeholder for a batch of input images.\n",
    "`y` is a placeholder for a batch of output labels.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "Create a training pipeline that uses the model to classify MNIST data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "logits = LeNet(x)\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate how well the loss and accuracy of the model for a given dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    total_loss = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        loss, accuracy = sess.run([loss_operation, accuracy_operation], feed_dict={x: batch_x, y: batch_y, keep_prob:1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "        total_loss += (loss * len(batch_x))\n",
    "    return total_loss / num_examples, total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Run the training data through the training pipeline to train the model.\n",
    "\n",
    "Before each epoch, shuffle the training set.\n",
    "\n",
    "After each epoch, measure the loss and accuracy of the validation set.\n",
    "\n",
    "Save the model after training.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "save_dir = 'LeNetV5'\n",
    "out_dir = out_dir = os.path.abspath(os.path.join(os.path.curdir, \"../Traffic-Sign-Classifier-runs\", save_dir))\n",
    "if os.path.exists(out_dir):\n",
    "    shutil.rmtree(out_dir)\n",
    "train_summary_dir = os.path.join(out_dir, \"train\")        \n",
    "valid_summary_dir = os.path.join(out_dir, \"valid\")\n",
    "checkpoint_dir = os.path.join(out_dir, \"checkpoints\")\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir,\"model\")\n",
    "checkpoint_every = 100\n",
    "train_summary_every = 100\n",
    "valid_summary_every = 100\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "Batch 100 Train Loss: 1.564 Train Accuracy: 0.531\n",
      "\n",
      "valid writing\n",
      "EPOCH 1 Batch 100 ...\n",
      "Validation loss = 1.693\n",
      "Validation Accuracy = 0.486\n",
      "\n",
      "Batch 200 Train Loss: 0.533 Train Accuracy: 0.891\n",
      "\n",
      "valid writing\n",
      "EPOCH 1 Batch 200 ...\n",
      "Validation loss = 0.855\n",
      "Validation Accuracy = 0.760\n",
      "\n",
      "Batch 300 Train Loss: 0.343 Train Accuracy: 0.898\n",
      "\n",
      "valid writing\n",
      "EPOCH 1 Batch 300 ...\n",
      "Validation loss = 0.602\n",
      "Validation Accuracy = 0.837\n",
      "\n",
      "Batch 400 Train Loss: 0.241 Train Accuracy: 0.938\n",
      "\n",
      "valid writing\n",
      "EPOCH 1 Batch 400 ...\n",
      "Validation loss = 0.517\n",
      "Validation Accuracy = 0.875\n",
      "\n",
      "Batch 500 Train Loss: 0.190 Train Accuracy: 0.961\n",
      "\n",
      "valid writing\n",
      "EPOCH 1 Batch 500 ...\n",
      "Validation loss = 0.455\n",
      "Validation Accuracy = 0.878\n",
      "\n",
      "Batch 600 Train Loss: 0.122 Train Accuracy: 0.969\n",
      "\n",
      "valid writing\n",
      "EPOCH 1 Batch 600 ...\n",
      "Validation loss = 0.442\n",
      "Validation Accuracy = 0.874\n",
      "\n",
      "Batch 700 Train Loss: 0.136 Train Accuracy: 0.984\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 700 ...\n",
      "Validation loss = 0.409\n",
      "Validation Accuracy = 0.893\n",
      "\n",
      "Batch 800 Train Loss: 0.109 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 800 ...\n",
      "Validation loss = 0.372\n",
      "Validation Accuracy = 0.910\n",
      "\n",
      "Batch 900 Train Loss: 0.068 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 900 ...\n",
      "Validation loss = 0.363\n",
      "Validation Accuracy = 0.909\n",
      "\n",
      "Batch 1000 Train Loss: 0.075 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 1000 ...\n",
      "Validation loss = 0.330\n",
      "Validation Accuracy = 0.914\n",
      "\n",
      "Batch 1100 Train Loss: 0.083 Train Accuracy: 0.977\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 1100 ...\n",
      "Validation loss = 0.330\n",
      "Validation Accuracy = 0.912\n",
      "\n",
      "Batch 1200 Train Loss: 0.088 Train Accuracy: 0.969\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 1200 ...\n",
      "Validation loss = 0.339\n",
      "Validation Accuracy = 0.910\n",
      "\n",
      "Batch 1300 Train Loss: 0.079 Train Accuracy: 0.977\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 1300 ...\n",
      "Validation loss = 0.296\n",
      "Validation Accuracy = 0.925\n",
      "\n",
      "Batch 1400 Train Loss: 0.069 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 1400 ...\n",
      "Validation loss = 0.323\n",
      "Validation Accuracy = 0.912\n",
      "\n",
      "Batch 1500 Train Loss: 0.058 Train Accuracy: 0.984\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 1500 ...\n",
      "Validation loss = 0.305\n",
      "Validation Accuracy = 0.920\n",
      "\n",
      "Batch 1600 Train Loss: 0.050 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 1600 ...\n",
      "Validation loss = 0.257\n",
      "Validation Accuracy = 0.939\n",
      "\n",
      "Batch 1700 Train Loss: 0.072 Train Accuracy: 0.977\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 1700 ...\n",
      "Validation loss = 0.262\n",
      "Validation Accuracy = 0.929\n",
      "\n",
      "Batch 1800 Train Loss: 0.071 Train Accuracy: 0.984\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 1800 ...\n",
      "Validation loss = 0.275\n",
      "Validation Accuracy = 0.932\n",
      "\n",
      "Batch 1900 Train Loss: 0.062 Train Accuracy: 0.984\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 1900 ...\n",
      "Validation loss = 0.268\n",
      "Validation Accuracy = 0.928\n",
      "\n",
      "Batch 2000 Train Loss: 0.099 Train Accuracy: 0.984\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 2000 ...\n",
      "Validation loss = 0.254\n",
      "Validation Accuracy = 0.937\n",
      "\n",
      "Batch 2100 Train Loss: 0.084 Train Accuracy: 0.969\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2100 ...\n",
      "Validation loss = 0.249\n",
      "Validation Accuracy = 0.936\n",
      "\n",
      "Batch 2200 Train Loss: 0.029 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2200 ...\n",
      "Validation loss = 0.239\n",
      "Validation Accuracy = 0.941\n",
      "\n",
      "Batch 2300 Train Loss: 0.080 Train Accuracy: 0.977\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2300 ...\n",
      "Validation loss = 0.223\n",
      "Validation Accuracy = 0.942\n",
      "\n",
      "Batch 2400 Train Loss: 0.071 Train Accuracy: 0.984\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2400 ...\n",
      "Validation loss = 0.246\n",
      "Validation Accuracy = 0.939\n",
      "\n",
      "Batch 2500 Train Loss: 0.052 Train Accuracy: 0.984\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2500 ...\n",
      "Validation loss = 0.265\n",
      "Validation Accuracy = 0.929\n",
      "\n",
      "Batch 2600 Train Loss: 0.060 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2600 ...\n",
      "Validation loss = 0.229\n",
      "Validation Accuracy = 0.951\n",
      "\n",
      "Batch 2700 Train Loss: 0.053 Train Accuracy: 0.977\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2700 ...\n",
      "Validation loss = 0.230\n",
      "Validation Accuracy = 0.943\n",
      "\n",
      "Batch 2800 Train Loss: 0.028 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 5 Batch 2800 ...\n",
      "Validation loss = 0.255\n",
      "Validation Accuracy = 0.941\n",
      "\n",
      "Batch 2900 Train Loss: 0.043 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 5 Batch 2900 ...\n",
      "Validation loss = 0.230\n",
      "Validation Accuracy = 0.942\n",
      "\n",
      "Batch 3000 Train Loss: 0.026 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 5 Batch 3000 ...\n",
      "Validation loss = 0.225\n",
      "Validation Accuracy = 0.949\n",
      "\n",
      "Batch 3100 Train Loss: 0.031 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 5 Batch 3100 ...\n",
      "Validation loss = 0.235\n",
      "Validation Accuracy = 0.948\n",
      "\n",
      "Batch 3200 Train Loss: 0.041 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 5 Batch 3200 ...\n",
      "Validation loss = 0.254\n",
      "Validation Accuracy = 0.939\n",
      "\n",
      "Batch 3300 Train Loss: 0.022 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 5 Batch 3300 ...\n",
      "Validation loss = 0.226\n",
      "Validation Accuracy = 0.951\n",
      "\n",
      "Batch 3400 Train Loss: 0.042 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 3400 ...\n",
      "Validation loss = 0.244\n",
      "Validation Accuracy = 0.940\n",
      "\n",
      "Batch 3500 Train Loss: 0.022 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 3500 ...\n",
      "Validation loss = 0.217\n",
      "Validation Accuracy = 0.946\n",
      "\n",
      "Batch 3600 Train Loss: 0.008 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 3600 ...\n",
      "Validation loss = 0.227\n",
      "Validation Accuracy = 0.950\n",
      "\n",
      "Batch 3700 Train Loss: 0.027 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 3700 ...\n",
      "Validation loss = 0.227\n",
      "Validation Accuracy = 0.950\n",
      "\n",
      "Batch 3800 Train Loss: 0.051 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 3800 ...\n",
      "Validation loss = 0.209\n",
      "Validation Accuracy = 0.948\n",
      "\n",
      "Batch 3900 Train Loss: 0.011 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 3900 ...\n",
      "Validation loss = 0.211\n",
      "Validation Accuracy = 0.951\n",
      "\n",
      "Batch 4000 Train Loss: 0.028 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 4000 ...\n",
      "Validation loss = 0.219\n",
      "Validation Accuracy = 0.955\n",
      "\n",
      "Batch 4100 Train Loss: 0.011 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4100 ...\n",
      "Validation loss = 0.210\n",
      "Validation Accuracy = 0.951\n",
      "\n",
      "Batch 4200 Train Loss: 0.007 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4200 ...\n",
      "Validation loss = 0.201\n",
      "Validation Accuracy = 0.953\n",
      "\n",
      "Batch 4300 Train Loss: 0.030 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4300 ...\n",
      "Validation loss = 0.201\n",
      "Validation Accuracy = 0.955\n",
      "\n",
      "Batch 4400 Train Loss: 0.012 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4400 ...\n",
      "Validation loss = 0.205\n",
      "Validation Accuracy = 0.960\n",
      "\n",
      "Batch 4500 Train Loss: 0.013 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4500 ...\n",
      "Validation loss = 0.211\n",
      "Validation Accuracy = 0.952\n",
      "\n",
      "Batch 4600 Train Loss: 0.011 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4600 ...\n",
      "Validation loss = 0.233\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 4700 Train Loss: 0.016 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4700 ...\n",
      "Validation loss = 0.222\n",
      "Validation Accuracy = 0.946\n",
      "\n",
      "Batch 4800 Train Loss: 0.027 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 4800 ...\n",
      "Validation loss = 0.232\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 4900 Train Loss: 0.018 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 4900 ...\n",
      "Validation loss = 0.214\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "Batch 5000 Train Loss: 0.019 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 5000 ...\n",
      "Validation loss = 0.227\n",
      "Validation Accuracy = 0.953\n",
      "\n",
      "Batch 5100 Train Loss: 0.025 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 5100 ...\n",
      "Validation loss = 0.226\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 5200 Train Loss: 0.022 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 5200 ...\n",
      "Validation loss = 0.194\n",
      "Validation Accuracy = 0.959\n",
      "\n",
      "Batch 5300 Train Loss: 0.009 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 5300 ...\n",
      "Validation loss = 0.224\n",
      "Validation Accuracy = 0.947\n",
      "\n",
      "Batch 5400 Train Loss: 0.003 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 5400 ...\n",
      "Validation loss = 0.199\n",
      "Validation Accuracy = 0.952\n",
      "\n",
      "Batch 5500 Train Loss: 0.017 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 9 Batch 5500 ...\n",
      "Validation loss = 0.215\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 5600 Train Loss: 0.007 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 9 Batch 5600 ...\n",
      "Validation loss = 0.197\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "Batch 5700 Train Loss: 0.007 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 9 Batch 5700 ...\n",
      "Validation loss = 0.206\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 5800 Train Loss: 0.010 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 9 Batch 5800 ...\n",
      "Validation loss = 0.213\n",
      "Validation Accuracy = 0.957\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5900 Train Loss: 0.016 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 9 Batch 5900 ...\n",
      "Validation loss = 0.213\n",
      "Validation Accuracy = 0.955\n",
      "\n",
      "Batch 6000 Train Loss: 0.021 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 9 Batch 6000 ...\n",
      "Validation loss = 0.202\n",
      "Validation Accuracy = 0.955\n",
      "\n",
      "Batch 6100 Train Loss: 0.011 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6100 ...\n",
      "Validation loss = 0.219\n",
      "Validation Accuracy = 0.951\n",
      "\n",
      "Batch 6200 Train Loss: 0.015 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6200 ...\n",
      "Validation loss = 0.211\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "Batch 6300 Train Loss: 0.039 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6300 ...\n",
      "Validation loss = 0.220\n",
      "Validation Accuracy = 0.953\n",
      "\n",
      "Batch 6400 Train Loss: 0.024 Train Accuracy: 0.984\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6400 ...\n",
      "Validation loss = 0.265\n",
      "Validation Accuracy = 0.944\n",
      "\n",
      "Batch 6500 Train Loss: 0.020 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6500 ...\n",
      "Validation loss = 0.199\n",
      "Validation Accuracy = 0.952\n",
      "\n",
      "Batch 6600 Train Loss: 0.005 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6600 ...\n",
      "Validation loss = 0.199\n",
      "Validation Accuracy = 0.959\n",
      "\n",
      "Batch 6700 Train Loss: 0.011 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6700 ...\n",
      "Validation loss = 0.201\n",
      "Validation Accuracy = 0.958\n",
      "\n",
      "Batch 6800 Train Loss: 0.019 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 6800 ...\n",
      "Validation loss = 0.205\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "Batch 6900 Train Loss: 0.006 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 6900 ...\n",
      "Validation loss = 0.214\n",
      "Validation Accuracy = 0.959\n",
      "\n",
      "Batch 7000 Train Loss: 0.025 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 7000 ...\n",
      "Validation loss = 0.184\n",
      "Validation Accuracy = 0.966\n",
      "\n",
      "Batch 7100 Train Loss: 0.010 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 7100 ...\n",
      "Validation loss = 0.232\n",
      "Validation Accuracy = 0.950\n",
      "\n",
      "Batch 7200 Train Loss: 0.037 Train Accuracy: 0.984\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 7200 ...\n",
      "Validation loss = 0.209\n",
      "Validation Accuracy = 0.959\n",
      "\n",
      "Batch 7300 Train Loss: 0.021 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 7300 ...\n",
      "Validation loss = 0.218\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 7400 Train Loss: 0.013 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 7400 ...\n",
      "Validation loss = 0.222\n",
      "Validation Accuracy = 0.945\n",
      "\n",
      "Batch 7500 Train Loss: 0.009 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 7500 ...\n",
      "Validation loss = 0.208\n",
      "Validation Accuracy = 0.949\n",
      "\n",
      "Batch 7600 Train Loss: 0.011 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 7600 ...\n",
      "Validation loss = 0.198\n",
      "Validation Accuracy = 0.963\n",
      "\n",
      "Batch 7700 Train Loss: 0.021 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 7700 ...\n",
      "Validation loss = 0.213\n",
      "Validation Accuracy = 0.955\n",
      "\n",
      "Batch 7800 Train Loss: 0.004 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 7800 ...\n",
      "Validation loss = 0.236\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 7900 Train Loss: 0.016 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 7900 ...\n",
      "Validation loss = 0.210\n",
      "Validation Accuracy = 0.952\n",
      "\n",
      "Batch 8000 Train Loss: 0.011 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 8000 ...\n",
      "Validation loss = 0.204\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "Batch 8100 Train Loss: 0.005 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 8100 ...\n",
      "Validation loss = 0.217\n",
      "Validation Accuracy = 0.952\n",
      "\n",
      "Batch 8200 Train Loss: 0.009 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 13 Batch 8200 ...\n",
      "Validation loss = 0.225\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 8300 Train Loss: 0.014 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 13 Batch 8300 ...\n",
      "Validation loss = 0.222\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "Batch 8400 Train Loss: 0.001 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 13 Batch 8400 ...\n",
      "Validation loss = 0.220\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 8500 Train Loss: 0.008 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 13 Batch 8500 ...\n",
      "Validation loss = 0.195\n",
      "Validation Accuracy = 0.961\n",
      "\n",
      "Batch 8600 Train Loss: 0.010 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 13 Batch 8600 ...\n",
      "Validation loss = 0.204\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "Batch 8700 Train Loss: 0.009 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 13 Batch 8700 ...\n",
      "Validation loss = 0.204\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "Batch 8800 Train Loss: 0.025 Train Accuracy: 0.984\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 8800 ...\n",
      "Validation loss = 0.227\n",
      "Validation Accuracy = 0.955\n",
      "\n",
      "Batch 8900 Train Loss: 0.019 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 8900 ...\n",
      "Validation loss = 0.206\n",
      "Validation Accuracy = 0.962\n",
      "\n",
      "Batch 9000 Train Loss: 0.017 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 9000 ...\n",
      "Validation loss = 0.190\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "Batch 9100 Train Loss: 0.013 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 9100 ...\n",
      "Validation loss = 0.216\n",
      "Validation Accuracy = 0.958\n",
      "\n",
      "Batch 9200 Train Loss: 0.013 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 9200 ...\n",
      "Validation loss = 0.197\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 9300 Train Loss: 0.011 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 9300 ...\n",
      "Validation loss = 0.229\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "Batch 9400 Train Loss: 0.004 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 9400 ...\n",
      "Validation loss = 0.207\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "Batch 9500 Train Loss: 0.003 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 9500 ...\n",
      "Validation loss = 0.200\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "Batch 9600 Train Loss: 0.003 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 9600 ...\n",
      "Validation loss = 0.197\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 9700 Train Loss: 0.017 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 9700 ...\n",
      "Validation loss = 0.228\n",
      "Validation Accuracy = 0.953\n",
      "\n",
      "Batch 9800 Train Loss: 0.010 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 9800 ...\n",
      "Validation loss = 0.226\n",
      "Validation Accuracy = 0.953\n",
      "\n",
      "Batch 9900 Train Loss: 0.022 Train Accuracy: 0.984\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 9900 ...\n",
      "Validation loss = 0.204\n",
      "Validation Accuracy = 0.949\n",
      "\n",
      "Batch 10000 Train Loss: 0.013 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 10000 ...\n",
      "Validation loss = 0.204\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 10100 Train Loss: 0.003 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 10100 ...\n",
      "Validation loss = 0.233\n",
      "Validation Accuracy = 0.950\n",
      "\n",
      "Batch 10200 Train Loss: 0.008 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10200 ...\n",
      "Validation loss = 0.196\n",
      "Validation Accuracy = 0.950\n",
      "\n",
      "Batch 10300 Train Loss: 0.019 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10300 ...\n",
      "Validation loss = 0.190\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "Batch 10400 Train Loss: 0.014 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10400 ...\n",
      "Validation loss = 0.203\n",
      "Validation Accuracy = 0.958\n",
      "\n",
      "Batch 10500 Train Loss: 0.023 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10500 ...\n",
      "Validation loss = 0.203\n",
      "Validation Accuracy = 0.961\n",
      "\n",
      "Batch 10600 Train Loss: 0.009 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10600 ...\n",
      "Validation loss = 0.209\n",
      "Validation Accuracy = 0.959\n",
      "\n",
      "Batch 10700 Train Loss: 0.024 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10700 ...\n",
      "Validation loss = 0.206\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "Batch 10800 Train Loss: 0.008 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10800 ...\n",
      "Validation loss = 0.231\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "Batch 10900 Train Loss: 0.017 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 17 Batch 10900 ...\n",
      "Validation loss = 0.224\n",
      "Validation Accuracy = 0.958\n",
      "\n",
      "Batch 11000 Train Loss: 0.012 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 17 Batch 11000 ...\n",
      "Validation loss = 0.235\n",
      "Validation Accuracy = 0.953\n",
      "\n",
      "Batch 11100 Train Loss: 0.005 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 17 Batch 11100 ...\n",
      "Validation loss = 0.221\n",
      "Validation Accuracy = 0.951\n",
      "\n",
      "Batch 11200 Train Loss: 0.015 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 17 Batch 11200 ...\n",
      "Validation loss = 0.217\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "Batch 11300 Train Loss: 0.008 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 17 Batch 11300 ...\n",
      "Validation loss = 0.229\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 11400 Train Loss: 0.035 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 17 Batch 11400 ...\n",
      "Validation loss = 0.236\n",
      "Validation Accuracy = 0.953\n",
      "\n",
      "Batch 11500 Train Loss: 0.032 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 11500 ...\n",
      "Validation loss = 0.206\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "Batch 11600 Train Loss: 0.003 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 11600 ...\n",
      "Validation loss = 0.219\n",
      "Validation Accuracy = 0.959\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11700 Train Loss: 0.004 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 11700 ...\n",
      "Validation loss = 0.225\n",
      "Validation Accuracy = 0.959\n",
      "\n",
      "Batch 11800 Train Loss: 0.015 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 11800 ...\n",
      "Validation loss = 0.236\n",
      "Validation Accuracy = 0.951\n",
      "\n",
      "Batch 11900 Train Loss: 0.004 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 11900 ...\n",
      "Validation loss = 0.229\n",
      "Validation Accuracy = 0.959\n",
      "\n",
      "Batch 12000 Train Loss: 0.003 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 12000 ...\n",
      "Validation loss = 0.230\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 12100 Train Loss: 0.031 Train Accuracy: 0.984\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 12100 ...\n",
      "Validation loss = 0.228\n",
      "Validation Accuracy = 0.958\n",
      "\n",
      "Batch 12200 Train Loss: 0.002 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12200 ...\n",
      "Validation loss = 0.230\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "Batch 12300 Train Loss: 0.005 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12300 ...\n",
      "Validation loss = 0.224\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "Batch 12400 Train Loss: 0.004 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12400 ...\n",
      "Validation loss = 0.233\n",
      "Validation Accuracy = 0.959\n",
      "\n",
      "Batch 12500 Train Loss: 0.002 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12500 ...\n",
      "Validation loss = 0.223\n",
      "Validation Accuracy = 0.958\n",
      "\n",
      "Batch 12600 Train Loss: 0.010 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12600 ...\n",
      "Validation loss = 0.213\n",
      "Validation Accuracy = 0.959\n",
      "\n",
      "Batch 12700 Train Loss: 0.003 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12700 ...\n",
      "Validation loss = 0.200\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "Batch 12800 Train Loss: 0.004 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12800 ...\n",
      "Validation loss = 0.257\n",
      "Validation Accuracy = 0.948\n",
      "\n",
      "Batch 12900 Train Loss: 0.005 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 12900 ...\n",
      "Validation loss = 0.237\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "Batch 13000 Train Loss: 0.007 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 13000 ...\n",
      "Validation loss = 0.217\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "Batch 13100 Train Loss: 0.003 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 13100 ...\n",
      "Validation loss = 0.232\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "Batch 13200 Train Loss: 0.004 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 13200 ...\n",
      "Validation loss = 0.209\n",
      "Validation Accuracy = 0.947\n",
      "\n",
      "Batch 13300 Train Loss: 0.020 Train Accuracy: 0.992\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 13300 ...\n",
      "Validation loss = 0.210\n",
      "Validation Accuracy = 0.957\n",
      "\n",
      "Batch 13400 Train Loss: 0.009 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 13400 ...\n",
      "Validation loss = 0.219\n",
      "Validation Accuracy = 0.952\n",
      "\n",
      "Batch 13500 Train Loss: 0.007 Train Accuracy: 1.000\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 13500 ...\n",
      "Validation loss = 0.257\n",
      "Validation Accuracy = 0.954\n",
      "\n",
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    global_step = 0\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "    valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob:0.7})\n",
    "            global_step += 1\n",
    "            if global_step % train_summary_every == 0:\n",
    "                train_loss, train_accuracy = evaluate(batch_x, batch_y)\n",
    "                train_summaries = tf.Summary()\n",
    "                train_summaries.value.add(tag='Train Loss', simple_value=train_loss)\n",
    "                train_summaries.value.add(tag='Train Accuracy', simple_value=train_accuracy)\n",
    "                train_summary_writer.add_summary(train_summaries, global_step)\n",
    "                print(\"Batch {} Train Loss: {:.3f} Train Accuracy: {:.3f}\".format(global_step, train_loss, train_accuracy))\n",
    "                print()\n",
    "            if global_step % valid_summary_every == 0:\n",
    "                validation_loss, validation_accuracy = evaluate(X_valid, y_valid)\n",
    "                valid_summaries = tf.Summary()\n",
    "                valid_summaries.value.add(tag='Validation Loss', simple_value=validation_loss)\n",
    "                valid_summaries.value.add(tag='Validation Accuracy', simple_value=validation_accuracy)\n",
    "                print(\"valid writing\")\n",
    "                valid_summary_writer.add_summary(valid_summaries, global_step)\n",
    "                print(\"EPOCH {} Batch {} ...\".format(i+1, global_step))\n",
    "                print(\"Validation loss = {:.3f}\".format(validation_loss))\n",
    "                print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "                print()\n",
    "            if global_step % checkpoint_every == 0:\n",
    "                saver.save(sess, checkpoint_prefix, global_step=global_step)            \n",
    "    print(\"Training Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "Once you are completely satisfied with your model, evaluate the performance of the model on the test set.\n",
    "\n",
    "Be sure to only do this once!\n",
    "\n",
    "If you were to measure the performance of your trained model on the test set, then improve your model, and then measure the performance of your model on the test set again, that would invalidate your test results. You wouldn't get a true measure of how well your model would perform against real data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.009\n",
      "Train Accuracy = 0.998\n",
      "Valid Loss = 0.257\n",
      "Valid Accuracy = 0.954\n",
      "Test Loss = 0.201\n",
      "Test Accuracy = 0.947\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    train_loss, train_accuracy = evaluate(X_train, y_train)\n",
    "    print(\"Train Loss = {:.3f}\".format(train_loss))\n",
    "    print(\"Train Accuracy = {:.3f}\".format(train_accuracy))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    valid_loss, valid_accuracy = evaluate(X_valid, y_valid)\n",
    "    print(\"Valid Loss = {:.3f}\".format(valid_loss))\n",
    "    print(\"Valid Accuracy = {:.3f}\".format(valid_accuracy))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    test_loss, test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Loss = {:.3f}\".format(test_loss))\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corr_predict(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    corr_predict = np.zeros(num_examples)\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        temp = np.array(sess.run([correct_prediction], feed_dict={x: batch_x, y: batch_y, keep_prob:1.0})).astype('Int32')\n",
    "        corr_predict[offset:offset+BATCH_SIZE] = temp\n",
    "    return corr_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    corr_pred = corr_predict(X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2fa6df9be0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAE7CAYAAACR9BMWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f30012e6518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4IAAAE7CAYAAACIUz/1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9RSUQ4ygPQkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f30007895f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "print(np.sum(corr_pred == 0))\n",
    "wrong_pred_indexes = np.argwhere(corr_pred == 0)\n",
    "plt.figure(figsize = (15,5))\n",
    "sns.countplot(y_valid[corr_pred == 0])\n",
    "plt.figure(figsize = (15,5))\n",
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let me augment the undersampled data and see what happens"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet for Traffic Sign V5\n",
    "![LeNet Architecture](lenet.png)\n",
    "Modified from source: Yan LeCun\n",
    "\n",
    "Author: Peng \"Patrick\" Su\n",
    "Augmenting the under-sampled data, to make it better-balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the traffic sign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = '../traffic-signs-data/train.p'\n",
    "validation_file = '../traffic-signs-data/valid.p'\n",
    "testing_file = '../traffic-signs-data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traffic sign data comes as 32x32x3 images, and LeNet accepts 32x32xC images. No need to pad anything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Shuffle the training data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 2 has the max amount of data, 2010\n",
      "Label 0 has the min amount of data, 180\n",
      "Next, Let's augment the undersampled classes to the max count\n"
     ]
    }
   ],
   "source": [
    "y_train_DF = pd.DataFrame(y_train, columns=['label'])\n",
    "group_count = y_train_DF.groupby(['label']).size()\n",
    "print('Label {} has the max amount of data, {}'.format(np.argmax(group_count), np.max(group_count)))\n",
    "print('Label {} has the min amount of data, {}'.format(np.argmin(group_count), np.min(group_count)))\n",
    "print(\"Next, Let's augment the undersampled classes to the max count\")\n",
    "max_count = np.max(group_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage import transform, filters, exposure\n",
    "import cv2\n",
    "def augment_image(img,angle_range=(-5, 5),shear_range=10,trans_range=5,brightness=0):\n",
    "    # rotation\n",
    "    angle = np.random.uniform(*angle_range)\n",
    "    rows, cols, _ = img.shape\n",
    "    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2), angle, 1)\n",
    "    # Translation\n",
    "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "    tr_y = trans_range*np.random.uniform()-trans_range/2\n",
    "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "    # Shear\n",
    "    pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "    pt1 = 5+shear_range*np.random.uniform()-shear_range/2\n",
    "    pt2 = 20+shear_range*np.random.uniform()-shear_range/2\n",
    "    # Brightness\n",
    "    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
    "    shear_M = cv2.getAffineTransform(pts1,pts2)\n",
    "\n",
    "    img = cv2.warpAffine(img,Rot_M,(cols,rows))\n",
    "    img = cv2.warpAffine(img,Trans_M,(cols,rows))\n",
    "    img = cv2.warpAffine(img,shear_M,(cols,rows))\n",
    "\n",
    "    if brightness == 1:\n",
    "        img = augment_brightness_camera_images(img)\n",
    "    \n",
    "    return img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X, y, group_count):\n",
    "    augmented_X = np.zeros([max_count*43, *X.shape[1:]], dtype = np.uint8)\n",
    "    augmented_y = np.zeros([max_count*43, *y.shape[1:]], dtype = np.uint8)\n",
    "    for label,count in enumerate(group_count):\n",
    "        perLabelData = X[y==label]\n",
    "        augmented_X[label*max_count:label*max_count+count] = perLabelData\n",
    "        for i in range(max_count - count):\n",
    "            ind = np.random.randint(0, count)\n",
    "            new_img = augment_image(perLabelData[ind])\n",
    "            augmented_X[label*max_count+count+i] = new_img\n",
    "        augmented_y[label*max_count:label*max_count+max_count] = label  \n",
    "    return augmented_X, augmented_y\n",
    "X_train, y_train = augment_data(X_train, y_train, group_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    return (img - 128.)/128.\n",
    "def hist_eq(img):\n",
    "    if (len(img.shape)>3): # if you're passing in a collection of images\n",
    "        num_images = img.shape[0]\n",
    "        image_shape = img.shape[1:]\n",
    "        locEqImg = np.zeros([num_images,image_shape[0],image_shape[1],image_shape[2]])\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(6,6))\n",
    "        for i in range(num_images):\n",
    "            currImg = img[i].squeeze()\n",
    "            img_lab = cv2.cvtColor(currImg, cv2.COLOR_RGB2LAB)\n",
    "            img_lab[:,:,0]=clahe.apply(img_lab[:,:,0])\n",
    "            tmp = cv2.cvtColor(img_lab, cv2.COLOR_LAB2RGB)\n",
    "            locEqImg[i] = tmp\n",
    "    else:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(6,6))\n",
    "        img_lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        img_lab[:,:,0]=clahe.apply(img_lab[:,:,0])\n",
    "        tmp = cv2.cvtColor(img_lab, cv2.COLOR_LAB2RGB)\n",
    "        locEqImg = tmp\n",
    "    return locEqImg\n",
    "\n",
    "X_train = hist_eq(X_train)\n",
    "X_train = normalize(X_train)\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "X_valid = hist_eq(X_valid)\n",
    "X_valid = normalize(X_valid)\n",
    "X_valid = X_valid.astype('float32')\n",
    "\n",
    "X_test = hist_eq(X_test)\n",
    "X_test = normalize(X_test)\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86430, 32, 32, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TensorFlow\n",
    "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implement LeNet-5\n",
    "Implement the [LeNet-5](http://yann.lecun.com/exdb/lenet/) neural network architecture.\n",
    "\n",
    "This is the only cell you need to edit.\n",
    "### Input\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "### Architecture\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using `tf.contrib.layers.flatten`, which is already imported for you.\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 5: Fully Connected (Logits).** This should have 10 outputs.\n",
    "\n",
    "### Output\n",
    "Return the result of the 2nd fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    weights = {\n",
    "        'wc1': tf.Variable(tf.truncated_normal([5,5,3,6], mu, sigma)),\n",
    "        'wc2': tf.Variable(tf.truncated_normal([5,5,6,16], mu, sigma)),\n",
    "        'wf3': tf.Variable(tf.truncated_normal([5*5*16, 120], mu, sigma)),\n",
    "        'wf4': tf.Variable(tf.truncated_normal([120, 84], mu, sigma)),\n",
    "        'wf5': tf.Variable(tf.truncated_normal([84, 43], mu, sigma))}\n",
    "    biases = {\n",
    "        'bc1': tf.zeros([6]),\n",
    "        'bc2': tf.zeros([16]),\n",
    "        'bf3': tf.zeros([120]),\n",
    "        'bf4': tf.zeros([84]),\n",
    "        'bf5': tf.zeros([43]),\n",
    "    }\n",
    "    # TODO: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1 = tf.nn.conv2d(x, weights['wc1'], strides = [1, 1, 1, 1], padding = 'VALID')\n",
    "    conv1 = tf.nn.bias_add(conv1, biases['bc1'])\n",
    "    # TODO: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)    \n",
    "    # TODO: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'VALID')\n",
    "    conv1 = tf.nn.dropout(conv1, keep_prob)\n",
    "#     flat1 = flatten(conv1, [-1, 14*14*6])\n",
    "    \n",
    "    # TODO: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2 = tf.nn.conv2d(conv1, weights['wc2'], strides = [1, 1, 1, 1], padding = 'VALID')\n",
    "    conv2 = tf.nn.bias_add(conv2, biases['bc2'])    \n",
    "    # TODO: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    # TODO: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'VALID')\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "    # TODO: Flatten. Input = 5x5x8. Output = 200.\n",
    "\n",
    "#     flat2 = flatten(conv2, [-1, 5*5*16])\n",
    "#     flat = tf.concat(1, [flat1, flat2])\n",
    "    \n",
    "    flat = flatten(conv2, [-1, 5*5*16])\n",
    "    \n",
    "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc3 = tf.add(tf.matmul(flat, weights['wf3']), biases['bf3'])\n",
    "    # TODO: Activation.\n",
    "    fc3 = tf.nn.relu(fc3)\n",
    "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc4 = tf.add(tf.matmul(fc3, weights['wf4']), biases['bf4'])\n",
    "    # TODO: Activation.\n",
    "    fc4 = tf.nn.relu(fc4)\n",
    "    # TODO: Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    logits = tf.add(tf.matmul(fc4, weights['wf5']), biases['bf5'])\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "Train LeNet to classify traffic sign data.\n",
    "\n",
    "`x` is a placeholder for a batch of input images.\n",
    "`y` is a placeholder for a batch of output labels.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "Create a training pipeline that uses the model to classify MNIST data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "logits = LeNet(x)\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate how well the loss and accuracy of the model for a given dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    total_loss = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        loss, accuracy = sess.run([loss_operation, accuracy_operation], feed_dict={x: batch_x, y: batch_y, keep_prob:1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "        total_loss += (loss * len(batch_x))\n",
    "    return total_loss / num_examples, total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Run the training data through the training pipeline to train the model.\n",
    "\n",
    "Before each epoch, shuffle the training set.\n",
    "\n",
    "After each epoch, measure the loss and accuracy of the validation set.\n",
    "\n",
    "Save the model after training.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "save_dir = 'LeNetV5'\n",
    "out_dir = out_dir = os.path.abspath(os.path.join(os.path.curdir, \"../Traffic-Sign-Classifier-runs\", save_dir))\n",
    "if os.path.exists(out_dir):\n",
    "    shutil.rmtree(out_dir)\n",
    "train_summary_dir = os.path.join(out_dir, \"train\")        \n",
    "valid_summary_dir = os.path.join(out_dir, \"valid\")\n",
    "checkpoint_dir = os.path.join(out_dir, \"checkpoints\")\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir,\"model\")\n",
    "checkpoint_every = 100\n",
    "train_summary_every = 100\n",
    "valid_summary_every = 100\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "Batch 100 Train Loss: 2.848 Train Accuracy: 0.234\n",
      "\n",
      "valid writing\n",
      "EPOCH 1 Batch 100 ...\n",
      "Validation loss = 2.504\n",
      "Validation Accuracy = 0.302\n",
      "\n",
      "Batch 200 Train Loss: 2.158 Train Accuracy: 0.359\n",
      "\n",
      "valid writing\n",
      "EPOCH 1 Batch 200 ...\n",
      "Validation loss = 1.726\n",
      "Validation Accuracy = 0.507\n",
      "\n",
      "Batch 300 Train Loss: 1.809 Train Accuracy: 0.523\n",
      "\n",
      "valid writing\n",
      "EPOCH 1 Batch 300 ...\n",
      "Validation loss = 1.372\n",
      "Validation Accuracy = 0.617\n",
      "\n",
      "Batch 400 Train Loss: 1.485 Train Accuracy: 0.625\n",
      "\n",
      "valid writing\n",
      "EPOCH 1 Batch 400 ...\n",
      "Validation loss = 1.166\n",
      "Validation Accuracy = 0.681\n",
      "\n",
      "Batch 500 Train Loss: 1.552 Train Accuracy: 0.586\n",
      "\n",
      "valid writing\n",
      "EPOCH 1 Batch 500 ...\n",
      "Validation loss = 0.983\n",
      "Validation Accuracy = 0.720\n",
      "\n",
      "Batch 600 Train Loss: 1.272 Train Accuracy: 0.641\n",
      "\n",
      "valid writing\n",
      "EPOCH 1 Batch 600 ...\n",
      "Validation loss = 0.896\n",
      "Validation Accuracy = 0.753\n",
      "\n",
      "Batch 700 Train Loss: 1.083 Train Accuracy: 0.719\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 700 ...\n",
      "Validation loss = 0.807\n",
      "Validation Accuracy = 0.774\n",
      "\n",
      "Batch 800 Train Loss: 1.138 Train Accuracy: 0.695\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 800 ...\n",
      "Validation loss = 0.731\n",
      "Validation Accuracy = 0.804\n",
      "\n",
      "Batch 900 Train Loss: 1.032 Train Accuracy: 0.750\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 900 ...\n",
      "Validation loss = 0.681\n",
      "Validation Accuracy = 0.810\n",
      "\n",
      "Batch 1000 Train Loss: 1.391 Train Accuracy: 0.633\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 1000 ...\n",
      "Validation loss = 0.670\n",
      "Validation Accuracy = 0.805\n",
      "\n",
      "Batch 1100 Train Loss: 1.026 Train Accuracy: 0.734\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 1100 ...\n",
      "Validation loss = 0.652\n",
      "Validation Accuracy = 0.817\n",
      "\n",
      "Batch 1200 Train Loss: 1.077 Train Accuracy: 0.672\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 1200 ...\n",
      "Validation loss = 0.558\n",
      "Validation Accuracy = 0.845\n",
      "\n",
      "Batch 1300 Train Loss: 0.787 Train Accuracy: 0.789\n",
      "\n",
      "valid writing\n",
      "EPOCH 2 Batch 1300 ...\n",
      "Validation loss = 0.531\n",
      "Validation Accuracy = 0.856\n",
      "\n",
      "Batch 1400 Train Loss: 0.834 Train Accuracy: 0.773\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 1400 ...\n",
      "Validation loss = 0.523\n",
      "Validation Accuracy = 0.846\n",
      "\n",
      "Batch 1500 Train Loss: 0.703 Train Accuracy: 0.820\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 1500 ...\n",
      "Validation loss = 0.485\n",
      "Validation Accuracy = 0.865\n",
      "\n",
      "Batch 1600 Train Loss: 0.690 Train Accuracy: 0.820\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 1600 ...\n",
      "Validation loss = 0.466\n",
      "Validation Accuracy = 0.871\n",
      "\n",
      "Batch 1700 Train Loss: 0.688 Train Accuracy: 0.844\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 1700 ...\n",
      "Validation loss = 0.439\n",
      "Validation Accuracy = 0.882\n",
      "\n",
      "Batch 1800 Train Loss: 0.671 Train Accuracy: 0.820\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 1800 ...\n",
      "Validation loss = 0.440\n",
      "Validation Accuracy = 0.885\n",
      "\n",
      "Batch 1900 Train Loss: 0.686 Train Accuracy: 0.789\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 1900 ...\n",
      "Validation loss = 0.404\n",
      "Validation Accuracy = 0.894\n",
      "\n",
      "Batch 2000 Train Loss: 0.655 Train Accuracy: 0.812\n",
      "\n",
      "valid writing\n",
      "EPOCH 3 Batch 2000 ...\n",
      "Validation loss = 0.429\n",
      "Validation Accuracy = 0.883\n",
      "\n",
      "Batch 2100 Train Loss: 0.724 Train Accuracy: 0.844\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2100 ...\n",
      "Validation loss = 0.395\n",
      "Validation Accuracy = 0.894\n",
      "\n",
      "Batch 2200 Train Loss: 0.473 Train Accuracy: 0.906\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2200 ...\n",
      "Validation loss = 0.415\n",
      "Validation Accuracy = 0.889\n",
      "\n",
      "Batch 2300 Train Loss: 0.621 Train Accuracy: 0.797\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2300 ...\n",
      "Validation loss = 0.363\n",
      "Validation Accuracy = 0.902\n",
      "\n",
      "Batch 2400 Train Loss: 0.508 Train Accuracy: 0.875\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2400 ...\n",
      "Validation loss = 0.361\n",
      "Validation Accuracy = 0.902\n",
      "\n",
      "Batch 2500 Train Loss: 0.585 Train Accuracy: 0.852\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2500 ...\n",
      "Validation loss = 0.359\n",
      "Validation Accuracy = 0.902\n",
      "\n",
      "Batch 2600 Train Loss: 0.483 Train Accuracy: 0.844\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2600 ...\n",
      "Validation loss = 0.351\n",
      "Validation Accuracy = 0.898\n",
      "\n",
      "Batch 2700 Train Loss: 0.546 Train Accuracy: 0.867\n",
      "\n",
      "valid writing\n",
      "EPOCH 4 Batch 2700 ...\n",
      "Validation loss = 0.323\n",
      "Validation Accuracy = 0.915\n",
      "\n",
      "Batch 2800 Train Loss: 0.380 Train Accuracy: 0.906\n",
      "\n",
      "valid writing\n",
      "EPOCH 5 Batch 2800 ...\n",
      "Validation loss = 0.331\n",
      "Validation Accuracy = 0.909\n",
      "\n",
      "Batch 2900 Train Loss: 0.508 Train Accuracy: 0.875\n",
      "\n",
      "valid writing\n",
      "EPOCH 5 Batch 2900 ...\n",
      "Validation loss = 0.322\n",
      "Validation Accuracy = 0.908\n",
      "\n",
      "Batch 3000 Train Loss: 0.400 Train Accuracy: 0.898\n",
      "\n",
      "valid writing\n",
      "EPOCH 5 Batch 3000 ...\n",
      "Validation loss = 0.332\n",
      "Validation Accuracy = 0.902\n",
      "\n",
      "Batch 3100 Train Loss: 0.464 Train Accuracy: 0.883\n",
      "\n",
      "valid writing\n",
      "EPOCH 5 Batch 3100 ...\n",
      "Validation loss = 0.319\n",
      "Validation Accuracy = 0.913\n",
      "\n",
      "Batch 3200 Train Loss: 0.516 Train Accuracy: 0.875\n",
      "\n",
      "valid writing\n",
      "EPOCH 5 Batch 3200 ...\n",
      "Validation loss = 0.316\n",
      "Validation Accuracy = 0.915\n",
      "\n",
      "Batch 3300 Train Loss: 0.455 Train Accuracy: 0.891\n",
      "\n",
      "valid writing\n",
      "EPOCH 5 Batch 3300 ...\n",
      "Validation loss = 0.314\n",
      "Validation Accuracy = 0.913\n",
      "\n",
      "Batch 3400 Train Loss: 0.523 Train Accuracy: 0.852\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 3400 ...\n",
      "Validation loss = 0.286\n",
      "Validation Accuracy = 0.928\n",
      "\n",
      "Batch 3500 Train Loss: 0.401 Train Accuracy: 0.859\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 3500 ...\n",
      "Validation loss = 0.308\n",
      "Validation Accuracy = 0.918\n",
      "\n",
      "Batch 3600 Train Loss: 0.503 Train Accuracy: 0.852\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 3600 ...\n",
      "Validation loss = 0.297\n",
      "Validation Accuracy = 0.920\n",
      "\n",
      "Batch 3700 Train Loss: 0.285 Train Accuracy: 0.945\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 3700 ...\n",
      "Validation loss = 0.295\n",
      "Validation Accuracy = 0.918\n",
      "\n",
      "Batch 3800 Train Loss: 0.477 Train Accuracy: 0.836\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 3800 ...\n",
      "Validation loss = 0.314\n",
      "Validation Accuracy = 0.914\n",
      "\n",
      "Batch 3900 Train Loss: 0.431 Train Accuracy: 0.891\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 3900 ...\n",
      "Validation loss = 0.282\n",
      "Validation Accuracy = 0.923\n",
      "\n",
      "Batch 4000 Train Loss: 0.482 Train Accuracy: 0.875\n",
      "\n",
      "valid writing\n",
      "EPOCH 6 Batch 4000 ...\n",
      "Validation loss = 0.306\n",
      "Validation Accuracy = 0.913\n",
      "\n",
      "Batch 4100 Train Loss: 0.321 Train Accuracy: 0.898\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4100 ...\n",
      "Validation loss = 0.295\n",
      "Validation Accuracy = 0.915\n",
      "\n",
      "Batch 4200 Train Loss: 0.423 Train Accuracy: 0.875\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4200 ...\n",
      "Validation loss = 0.300\n",
      "Validation Accuracy = 0.911\n",
      "\n",
      "Batch 4300 Train Loss: 0.290 Train Accuracy: 0.922\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4300 ...\n",
      "Validation loss = 0.292\n",
      "Validation Accuracy = 0.916\n",
      "\n",
      "Batch 4400 Train Loss: 0.478 Train Accuracy: 0.875\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4400 ...\n",
      "Validation loss = 0.283\n",
      "Validation Accuracy = 0.931\n",
      "\n",
      "Batch 4500 Train Loss: 0.497 Train Accuracy: 0.859\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4500 ...\n",
      "Validation loss = 0.273\n",
      "Validation Accuracy = 0.915\n",
      "\n",
      "Batch 4600 Train Loss: 0.306 Train Accuracy: 0.922\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4600 ...\n",
      "Validation loss = 0.295\n",
      "Validation Accuracy = 0.918\n",
      "\n",
      "Batch 4700 Train Loss: 0.408 Train Accuracy: 0.891\n",
      "\n",
      "valid writing\n",
      "EPOCH 7 Batch 4700 ...\n",
      "Validation loss = 0.290\n",
      "Validation Accuracy = 0.918\n",
      "\n",
      "Batch 4800 Train Loss: 0.386 Train Accuracy: 0.914\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 4800 ...\n",
      "Validation loss = 0.301\n",
      "Validation Accuracy = 0.913\n",
      "\n",
      "Batch 4900 Train Loss: 0.293 Train Accuracy: 0.969\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 4900 ...\n",
      "Validation loss = 0.285\n",
      "Validation Accuracy = 0.923\n",
      "\n",
      "Batch 5000 Train Loss: 0.494 Train Accuracy: 0.867\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 5000 ...\n",
      "Validation loss = 0.292\n",
      "Validation Accuracy = 0.920\n",
      "\n",
      "Batch 5100 Train Loss: 0.351 Train Accuracy: 0.922\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 5100 ...\n",
      "Validation loss = 0.323\n",
      "Validation Accuracy = 0.907\n",
      "\n",
      "Batch 5200 Train Loss: 0.336 Train Accuracy: 0.930\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 5200 ...\n",
      "Validation loss = 0.284\n",
      "Validation Accuracy = 0.915\n",
      "\n",
      "Batch 5300 Train Loss: 0.453 Train Accuracy: 0.875\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 5300 ...\n",
      "Validation loss = 0.257\n",
      "Validation Accuracy = 0.924\n",
      "\n",
      "Batch 5400 Train Loss: 0.391 Train Accuracy: 0.875\n",
      "\n",
      "valid writing\n",
      "EPOCH 8 Batch 5400 ...\n",
      "Validation loss = 0.289\n",
      "Validation Accuracy = 0.920\n",
      "\n",
      "Batch 5500 Train Loss: 0.376 Train Accuracy: 0.891\n",
      "\n",
      "valid writing\n",
      "EPOCH 9 Batch 5500 ...\n",
      "Validation loss = 0.274\n",
      "Validation Accuracy = 0.919\n",
      "\n",
      "Batch 5600 Train Loss: 0.320 Train Accuracy: 0.883\n",
      "\n",
      "valid writing\n",
      "EPOCH 9 Batch 5600 ...\n",
      "Validation loss = 0.285\n",
      "Validation Accuracy = 0.920\n",
      "\n",
      "Batch 5700 Train Loss: 0.280 Train Accuracy: 0.922\n",
      "\n",
      "valid writing\n",
      "EPOCH 9 Batch 5700 ...\n",
      "Validation loss = 0.314\n",
      "Validation Accuracy = 0.911\n",
      "\n",
      "Batch 5800 Train Loss: 0.338 Train Accuracy: 0.891\n",
      "\n",
      "valid writing\n",
      "EPOCH 9 Batch 5800 ...\n",
      "Validation loss = 0.294\n",
      "Validation Accuracy = 0.911\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5900 Train Loss: 0.376 Train Accuracy: 0.898\n",
      "\n",
      "valid writing\n",
      "EPOCH 9 Batch 5900 ...\n",
      "Validation loss = 0.270\n",
      "Validation Accuracy = 0.920\n",
      "\n",
      "Batch 6000 Train Loss: 0.402 Train Accuracy: 0.867\n",
      "\n",
      "valid writing\n",
      "EPOCH 9 Batch 6000 ...\n",
      "Validation loss = 0.287\n",
      "Validation Accuracy = 0.916\n",
      "\n",
      "Batch 6100 Train Loss: 0.266 Train Accuracy: 0.961\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6100 ...\n",
      "Validation loss = 0.268\n",
      "Validation Accuracy = 0.919\n",
      "\n",
      "Batch 6200 Train Loss: 0.356 Train Accuracy: 0.930\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6200 ...\n",
      "Validation loss = 0.287\n",
      "Validation Accuracy = 0.915\n",
      "\n",
      "Batch 6300 Train Loss: 0.341 Train Accuracy: 0.906\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6300 ...\n",
      "Validation loss = 0.259\n",
      "Validation Accuracy = 0.927\n",
      "\n",
      "Batch 6400 Train Loss: 0.322 Train Accuracy: 0.898\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6400 ...\n",
      "Validation loss = 0.264\n",
      "Validation Accuracy = 0.920\n",
      "\n",
      "Batch 6500 Train Loss: 0.332 Train Accuracy: 0.930\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6500 ...\n",
      "Validation loss = 0.310\n",
      "Validation Accuracy = 0.905\n",
      "\n",
      "Batch 6600 Train Loss: 0.194 Train Accuracy: 0.977\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6600 ...\n",
      "Validation loss = 0.262\n",
      "Validation Accuracy = 0.928\n",
      "\n",
      "Batch 6700 Train Loss: 0.415 Train Accuracy: 0.875\n",
      "\n",
      "valid writing\n",
      "EPOCH 10 Batch 6700 ...\n",
      "Validation loss = 0.269\n",
      "Validation Accuracy = 0.921\n",
      "\n",
      "Batch 6800 Train Loss: 0.379 Train Accuracy: 0.891\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 6800 ...\n",
      "Validation loss = 0.280\n",
      "Validation Accuracy = 0.928\n",
      "\n",
      "Batch 6900 Train Loss: 0.216 Train Accuracy: 0.953\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 6900 ...\n",
      "Validation loss = 0.282\n",
      "Validation Accuracy = 0.914\n",
      "\n",
      "Batch 7000 Train Loss: 0.257 Train Accuracy: 0.930\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 7000 ...\n",
      "Validation loss = 0.260\n",
      "Validation Accuracy = 0.925\n",
      "\n",
      "Batch 7100 Train Loss: 0.283 Train Accuracy: 0.922\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 7100 ...\n",
      "Validation loss = 0.277\n",
      "Validation Accuracy = 0.926\n",
      "\n",
      "Batch 7200 Train Loss: 0.438 Train Accuracy: 0.875\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 7200 ...\n",
      "Validation loss = 0.320\n",
      "Validation Accuracy = 0.912\n",
      "\n",
      "Batch 7300 Train Loss: 0.364 Train Accuracy: 0.883\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 7300 ...\n",
      "Validation loss = 0.281\n",
      "Validation Accuracy = 0.917\n",
      "\n",
      "Batch 7400 Train Loss: 0.413 Train Accuracy: 0.867\n",
      "\n",
      "valid writing\n",
      "EPOCH 11 Batch 7400 ...\n",
      "Validation loss = 0.292\n",
      "Validation Accuracy = 0.911\n",
      "\n",
      "Batch 7500 Train Loss: 0.217 Train Accuracy: 0.922\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 7500 ...\n",
      "Validation loss = 0.290\n",
      "Validation Accuracy = 0.922\n",
      "\n",
      "Batch 7600 Train Loss: 0.194 Train Accuracy: 0.961\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 7600 ...\n",
      "Validation loss = 0.289\n",
      "Validation Accuracy = 0.922\n",
      "\n",
      "Batch 7700 Train Loss: 0.275 Train Accuracy: 0.906\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 7700 ...\n",
      "Validation loss = 0.287\n",
      "Validation Accuracy = 0.914\n",
      "\n",
      "Batch 7800 Train Loss: 0.212 Train Accuracy: 0.969\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 7800 ...\n",
      "Validation loss = 0.248\n",
      "Validation Accuracy = 0.927\n",
      "\n",
      "Batch 7900 Train Loss: 0.326 Train Accuracy: 0.914\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 7900 ...\n",
      "Validation loss = 0.248\n",
      "Validation Accuracy = 0.922\n",
      "\n",
      "Batch 8000 Train Loss: 0.260 Train Accuracy: 0.938\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 8000 ...\n",
      "Validation loss = 0.235\n",
      "Validation Accuracy = 0.931\n",
      "\n",
      "Batch 8100 Train Loss: 0.307 Train Accuracy: 0.891\n",
      "\n",
      "valid writing\n",
      "EPOCH 12 Batch 8100 ...\n",
      "Validation loss = 0.285\n",
      "Validation Accuracy = 0.924\n",
      "\n",
      "Batch 8200 Train Loss: 0.314 Train Accuracy: 0.930\n",
      "\n",
      "valid writing\n",
      "EPOCH 13 Batch 8200 ...\n",
      "Validation loss = 0.247\n",
      "Validation Accuracy = 0.933\n",
      "\n",
      "Batch 8300 Train Loss: 0.232 Train Accuracy: 0.945\n",
      "\n",
      "valid writing\n",
      "EPOCH 13 Batch 8300 ...\n",
      "Validation loss = 0.282\n",
      "Validation Accuracy = 0.919\n",
      "\n",
      "Batch 8400 Train Loss: 0.259 Train Accuracy: 0.906\n",
      "\n",
      "valid writing\n",
      "EPOCH 13 Batch 8400 ...\n",
      "Validation loss = 0.251\n",
      "Validation Accuracy = 0.926\n",
      "\n",
      "Batch 8500 Train Loss: 0.337 Train Accuracy: 0.938\n",
      "\n",
      "valid writing\n",
      "EPOCH 13 Batch 8500 ...\n",
      "Validation loss = 0.251\n",
      "Validation Accuracy = 0.924\n",
      "\n",
      "Batch 8600 Train Loss: 0.245 Train Accuracy: 0.945\n",
      "\n",
      "valid writing\n",
      "EPOCH 13 Batch 8600 ...\n",
      "Validation loss = 0.254\n",
      "Validation Accuracy = 0.927\n",
      "\n",
      "Batch 8700 Train Loss: 0.209 Train Accuracy: 0.961\n",
      "\n",
      "valid writing\n",
      "EPOCH 13 Batch 8700 ...\n",
      "Validation loss = 0.245\n",
      "Validation Accuracy = 0.933\n",
      "\n",
      "Batch 8800 Train Loss: 0.283 Train Accuracy: 0.914\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 8800 ...\n",
      "Validation loss = 0.240\n",
      "Validation Accuracy = 0.932\n",
      "\n",
      "Batch 8900 Train Loss: 0.289 Train Accuracy: 0.914\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 8900 ...\n",
      "Validation loss = 0.242\n",
      "Validation Accuracy = 0.930\n",
      "\n",
      "Batch 9000 Train Loss: 0.219 Train Accuracy: 0.938\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 9000 ...\n",
      "Validation loss = 0.235\n",
      "Validation Accuracy = 0.934\n",
      "\n",
      "Batch 9100 Train Loss: 0.211 Train Accuracy: 0.930\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 9100 ...\n",
      "Validation loss = 0.268\n",
      "Validation Accuracy = 0.925\n",
      "\n",
      "Batch 9200 Train Loss: 0.217 Train Accuracy: 0.945\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 9200 ...\n",
      "Validation loss = 0.245\n",
      "Validation Accuracy = 0.927\n",
      "\n",
      "Batch 9300 Train Loss: 0.367 Train Accuracy: 0.898\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 9300 ...\n",
      "Validation loss = 0.252\n",
      "Validation Accuracy = 0.927\n",
      "\n",
      "Batch 9400 Train Loss: 0.249 Train Accuracy: 0.930\n",
      "\n",
      "valid writing\n",
      "EPOCH 14 Batch 9400 ...\n",
      "Validation loss = 0.257\n",
      "Validation Accuracy = 0.925\n",
      "\n",
      "Batch 9500 Train Loss: 0.227 Train Accuracy: 0.953\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 9500 ...\n",
      "Validation loss = 0.249\n",
      "Validation Accuracy = 0.933\n",
      "\n",
      "Batch 9600 Train Loss: 0.166 Train Accuracy: 0.945\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 9600 ...\n",
      "Validation loss = 0.235\n",
      "Validation Accuracy = 0.933\n",
      "\n",
      "Batch 9700 Train Loss: 0.224 Train Accuracy: 0.953\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 9700 ...\n",
      "Validation loss = 0.234\n",
      "Validation Accuracy = 0.936\n",
      "\n",
      "Batch 9800 Train Loss: 0.246 Train Accuracy: 0.930\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 9800 ...\n",
      "Validation loss = 0.267\n",
      "Validation Accuracy = 0.927\n",
      "\n",
      "Batch 9900 Train Loss: 0.194 Train Accuracy: 0.961\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 9900 ...\n",
      "Validation loss = 0.258\n",
      "Validation Accuracy = 0.926\n",
      "\n",
      "Batch 10000 Train Loss: 0.235 Train Accuracy: 0.938\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 10000 ...\n",
      "Validation loss = 0.265\n",
      "Validation Accuracy = 0.921\n",
      "\n",
      "Batch 10100 Train Loss: 0.232 Train Accuracy: 0.938\n",
      "\n",
      "valid writing\n",
      "EPOCH 15 Batch 10100 ...\n",
      "Validation loss = 0.234\n",
      "Validation Accuracy = 0.930\n",
      "\n",
      "Batch 10200 Train Loss: 0.273 Train Accuracy: 0.938\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10200 ...\n",
      "Validation loss = 0.240\n",
      "Validation Accuracy = 0.935\n",
      "\n",
      "Batch 10300 Train Loss: 0.339 Train Accuracy: 0.906\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10300 ...\n",
      "Validation loss = 0.249\n",
      "Validation Accuracy = 0.929\n",
      "\n",
      "Batch 10400 Train Loss: 0.176 Train Accuracy: 0.969\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10400 ...\n",
      "Validation loss = 0.261\n",
      "Validation Accuracy = 0.920\n",
      "\n",
      "Batch 10500 Train Loss: 0.224 Train Accuracy: 0.945\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10500 ...\n",
      "Validation loss = 0.266\n",
      "Validation Accuracy = 0.916\n",
      "\n",
      "Batch 10600 Train Loss: 0.228 Train Accuracy: 0.938\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10600 ...\n",
      "Validation loss = 0.233\n",
      "Validation Accuracy = 0.935\n",
      "\n",
      "Batch 10700 Train Loss: 0.190 Train Accuracy: 0.961\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10700 ...\n",
      "Validation loss = 0.250\n",
      "Validation Accuracy = 0.928\n",
      "\n",
      "Batch 10800 Train Loss: 0.294 Train Accuracy: 0.898\n",
      "\n",
      "valid writing\n",
      "EPOCH 16 Batch 10800 ...\n",
      "Validation loss = 0.228\n",
      "Validation Accuracy = 0.932\n",
      "\n",
      "Batch 10900 Train Loss: 0.184 Train Accuracy: 0.969\n",
      "\n",
      "valid writing\n",
      "EPOCH 17 Batch 10900 ...\n",
      "Validation loss = 0.271\n",
      "Validation Accuracy = 0.927\n",
      "\n",
      "Batch 11000 Train Loss: 0.264 Train Accuracy: 0.930\n",
      "\n",
      "valid writing\n",
      "EPOCH 17 Batch 11000 ...\n",
      "Validation loss = 0.260\n",
      "Validation Accuracy = 0.928\n",
      "\n",
      "Batch 11100 Train Loss: 0.292 Train Accuracy: 0.875\n",
      "\n",
      "valid writing\n",
      "EPOCH 17 Batch 11100 ...\n",
      "Validation loss = 0.263\n",
      "Validation Accuracy = 0.926\n",
      "\n",
      "Batch 11200 Train Loss: 0.230 Train Accuracy: 0.938\n",
      "\n",
      "valid writing\n",
      "EPOCH 17 Batch 11200 ...\n",
      "Validation loss = 0.245\n",
      "Validation Accuracy = 0.928\n",
      "\n",
      "Batch 11300 Train Loss: 0.158 Train Accuracy: 0.961\n",
      "\n",
      "valid writing\n",
      "EPOCH 17 Batch 11300 ...\n",
      "Validation loss = 0.252\n",
      "Validation Accuracy = 0.926\n",
      "\n",
      "Batch 11400 Train Loss: 0.192 Train Accuracy: 0.953\n",
      "\n",
      "valid writing\n",
      "EPOCH 17 Batch 11400 ...\n",
      "Validation loss = 0.250\n",
      "Validation Accuracy = 0.934\n",
      "\n",
      "Batch 11500 Train Loss: 0.184 Train Accuracy: 0.961\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 11500 ...\n",
      "Validation loss = 0.237\n",
      "Validation Accuracy = 0.933\n",
      "\n",
      "Batch 11600 Train Loss: 0.229 Train Accuracy: 0.922\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 11600 ...\n",
      "Validation loss = 0.243\n",
      "Validation Accuracy = 0.932\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11700 Train Loss: 0.156 Train Accuracy: 0.969\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 11700 ...\n",
      "Validation loss = 0.269\n",
      "Validation Accuracy = 0.921\n",
      "\n",
      "Batch 11800 Train Loss: 0.232 Train Accuracy: 0.938\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 11800 ...\n",
      "Validation loss = 0.239\n",
      "Validation Accuracy = 0.937\n",
      "\n",
      "Batch 11900 Train Loss: 0.217 Train Accuracy: 0.930\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 11900 ...\n",
      "Validation loss = 0.282\n",
      "Validation Accuracy = 0.924\n",
      "\n",
      "Batch 12000 Train Loss: 0.198 Train Accuracy: 0.945\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 12000 ...\n",
      "Validation loss = 0.284\n",
      "Validation Accuracy = 0.922\n",
      "\n",
      "Batch 12100 Train Loss: 0.260 Train Accuracy: 0.898\n",
      "\n",
      "valid writing\n",
      "EPOCH 18 Batch 12100 ...\n",
      "Validation loss = 0.253\n",
      "Validation Accuracy = 0.929\n",
      "\n",
      "Batch 12200 Train Loss: 0.238 Train Accuracy: 0.922\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12200 ...\n",
      "Validation loss = 0.257\n",
      "Validation Accuracy = 0.928\n",
      "\n",
      "Batch 12300 Train Loss: 0.278 Train Accuracy: 0.898\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12300 ...\n",
      "Validation loss = 0.267\n",
      "Validation Accuracy = 0.924\n",
      "\n",
      "Batch 12400 Train Loss: 0.237 Train Accuracy: 0.930\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12400 ...\n",
      "Validation loss = 0.268\n",
      "Validation Accuracy = 0.925\n",
      "\n",
      "Batch 12500 Train Loss: 0.226 Train Accuracy: 0.930\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12500 ...\n",
      "Validation loss = 0.264\n",
      "Validation Accuracy = 0.923\n",
      "\n",
      "Batch 12600 Train Loss: 0.201 Train Accuracy: 0.945\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12600 ...\n",
      "Validation loss = 0.240\n",
      "Validation Accuracy = 0.935\n",
      "\n",
      "Batch 12700 Train Loss: 0.203 Train Accuracy: 0.938\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12700 ...\n",
      "Validation loss = 0.289\n",
      "Validation Accuracy = 0.922\n",
      "\n",
      "Batch 12800 Train Loss: 0.173 Train Accuracy: 0.953\n",
      "\n",
      "valid writing\n",
      "EPOCH 19 Batch 12800 ...\n",
      "Validation loss = 0.251\n",
      "Validation Accuracy = 0.924\n",
      "\n",
      "Batch 12900 Train Loss: 0.278 Train Accuracy: 0.930\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 12900 ...\n",
      "Validation loss = 0.236\n",
      "Validation Accuracy = 0.935\n",
      "\n",
      "Batch 13000 Train Loss: 0.223 Train Accuracy: 0.938\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 13000 ...\n",
      "Validation loss = 0.217\n",
      "Validation Accuracy = 0.935\n",
      "\n",
      "Batch 13100 Train Loss: 0.144 Train Accuracy: 0.977\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 13100 ...\n",
      "Validation loss = 0.257\n",
      "Validation Accuracy = 0.937\n",
      "\n",
      "Batch 13200 Train Loss: 0.124 Train Accuracy: 0.977\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 13200 ...\n",
      "Validation loss = 0.258\n",
      "Validation Accuracy = 0.931\n",
      "\n",
      "Batch 13300 Train Loss: 0.238 Train Accuracy: 0.945\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 13300 ...\n",
      "Validation loss = 0.249\n",
      "Validation Accuracy = 0.933\n",
      "\n",
      "Batch 13400 Train Loss: 0.145 Train Accuracy: 0.961\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 13400 ...\n",
      "Validation loss = 0.283\n",
      "Validation Accuracy = 0.916\n",
      "\n",
      "Batch 13500 Train Loss: 0.178 Train Accuracy: 0.953\n",
      "\n",
      "valid writing\n",
      "EPOCH 20 Batch 13500 ...\n",
      "Validation loss = 0.235\n",
      "Validation Accuracy = 0.934\n",
      "\n",
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    global_step = 0\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "    valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob:0.7})\n",
    "            global_step += 1\n",
    "            if global_step % train_summary_every == 0:\n",
    "                train_loss, train_accuracy = evaluate(batch_x, batch_y)\n",
    "                train_summaries = tf.Summary()\n",
    "                train_summaries.value.add(tag='Train Loss', simple_value=train_loss)\n",
    "                train_summaries.value.add(tag='Train Accuracy', simple_value=train_accuracy)\n",
    "                train_summary_writer.add_summary(train_summaries, global_step)\n",
    "                print(\"Batch {} Train Loss: {:.3f} Train Accuracy: {:.3f}\".format(global_step, train_loss, train_accuracy))\n",
    "                print()\n",
    "            if global_step % valid_summary_every == 0:\n",
    "                validation_loss, validation_accuracy = evaluate(X_valid, y_valid)\n",
    "                valid_summaries = tf.Summary()\n",
    "                valid_summaries.value.add(tag='Validation Loss', simple_value=validation_loss)\n",
    "                valid_summaries.value.add(tag='Validation Accuracy', simple_value=validation_accuracy)\n",
    "                print(\"valid writing\")\n",
    "                valid_summary_writer.add_summary(valid_summaries, global_step)\n",
    "                print(\"EPOCH {} Batch {} ...\".format(i+1, global_step))\n",
    "                print(\"Validation loss = {:.3f}\".format(validation_loss))\n",
    "                print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "                print()\n",
    "            if global_step % checkpoint_every == 0:\n",
    "                saver.save(sess, checkpoint_prefix, global_step=global_step)            \n",
    "    print(\"Training Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "Once you are completely satisfied with your model, evaluate the performance of the model on the test set.\n",
    "\n",
    "Be sure to only do this once!\n",
    "\n",
    "If you were to measure the performance of your trained model on the test set, then improve your model, and then measure the performance of your model on the test set again, that would invalidate your test results. You wouldn't get a true measure of how well your model would perform against real data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.107\n",
      "Train Accuracy = 0.971\n",
      "Valid Loss = 0.290\n",
      "Valid Accuracy = 0.927\n",
      "Test Loss = 0.237\n",
      "Test Accuracy = 0.945\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    train_loss, train_accuracy = evaluate(X_train, y_train)\n",
    "    print(\"Train Loss = {:.3f}\".format(train_loss))\n",
    "    print(\"Train Accuracy = {:.3f}\".format(train_accuracy))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    valid_loss, valid_accuracy = evaluate(X_valid, y_valid)\n",
    "    print(\"Valid Loss = {:.3f}\".format(valid_loss))\n",
    "    print(\"Valid Accuracy = {:.3f}\".format(valid_accuracy))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    test_loss, test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Loss = {:.3f}\".format(test_loss))\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corr_predict(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    corr_predict = np.zeros(num_examples)\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        temp = np.array(sess.run([correct_prediction], feed_dict={x: batch_x, y: batch_y, keep_prob:1.0})).astype('Int32')\n",
    "        corr_predict[offset:offset+BATCH_SIZE] = temp\n",
    "    return corr_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    corr_pred = corr_predict(X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "print(np.sum(corr_pred == 0))\n",
    "wrong_pred_indexes = np.argwhere(corr_pred == 0)\n",
    "plt.figure(figsize = (15,5))\n",
    "sns.countplot(y_valid[corr_pred == 0])\n",
    "plt.figure(figsize = (15,5))\n",
    "sns.countplot(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let me augment the undersampled data and see what happens"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
